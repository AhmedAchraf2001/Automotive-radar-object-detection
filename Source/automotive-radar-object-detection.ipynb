{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8086793,"sourceType":"datasetVersion","datasetId":4773726}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:02:00.630405Z","iopub.execute_input":"2024-04-18T21:02:00.630758Z","iopub.status.idle":"2024-04-18T21:02:00.635189Z","shell.execute_reply.started":"2024-04-18T21:02:00.630734Z","shell.execute_reply":"2024-04-18T21:02:00.634106Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"li = [[],[],[]]\nfor i, subfolder in zip(range(3), ['RAD', 'gt', 'stereo_image']):\n    for filename in os.listdir('/kaggle/input/carrada/test/' + str(subfolder)):\n            li[i].append(os.path.join('/kaggle/input/carrada/test/' + str(subfolder), filename))\n            li[i].sort()\ndf = pd.DataFrame({'rad':li[0], 'gt':li[1], 'images':li[2]})\n\n\nimageid_bbox_label = [[], [], []]\nfor file in li[1]:\n    for x in ['classes', 'boxes']:\n        file_content = pd.read_pickle(file)\n        for i in range(len(file_content[x])):\n            if x == 'classes':\n                imageid_bbox_label[0].append(file.split('/')[-1][:-7])\n                imageid_bbox_label[1].append(file_content[x][i])  \n            else:\n                x1 = file_content[x][i][0] - int(0.5 * (file_content[x][i][3]))\n                y = file_content[x][i][1] - int(0.5 * (file_content[x][i][4]))\n                w = file_content[x][i][3] \n                h = file_content[x][i][4]\n                xywh = [x1, y, w, h]  #get only xyxy instead of xyzwhd\n                imageid_bbox_label[2].append(xywh)\ndf = pd.DataFrame(imageid_bbox_label).T\ndf = df.rename(columns={0:'imageid',1:'label',2:'bbox'})\ndata = df.copy()\nclasses = {'truck':4, 'car':2, 'person':3, 'bicycle':0, 'bus':1}\ndf.label.replace(classes, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:02:01.002859Z","iopub.execute_input":"2024-04-18T21:02:01.003254Z","iopub.status.idle":"2024-04-18T21:02:01.443346Z","shell.execute_reply.started":"2024-04-18T21:02:01.003224Z","shell.execute_reply":"2024-04-18T21:02:01.437443Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/905451680.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df.label.replace(classes, inplace=True)\n/tmp/ipykernel_34/905451680.py:28: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df.label.replace(classes, inplace=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"data.head(4) ","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:02:02.880096Z","iopub.execute_input":"2024-04-18T21:02:02.880997Z","iopub.status.idle":"2024-04-18T21:02:02.897790Z","shell.execute_reply.started":"2024-04-18T21:02:02.880964Z","shell.execute_reply":"2024-04-18T21:02:02.896885Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"  imageid  label                        bbox\n0  000090  truck   [132.5, 92.5, 28.0, 30.0]\n1  000090  truck    [16.0, 163.0, 9.0, 11.0]\n2  000156    car    [79.5, 163.0, 8.0, 11.0]\n3  000156    car  [175.0, 123.5, 27.0, 40.0]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imageid</th>\n      <th>label</th>\n      <th>bbox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000090</td>\n      <td>truck</td>\n      <td>[132.5, 92.5, 28.0, 30.0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000090</td>\n      <td>truck</td>\n      <td>[16.0, 163.0, 9.0, 11.0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000156</td>\n      <td>car</td>\n      <td>[79.5, 163.0, 8.0, 11.0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000156</td>\n      <td>car</td>\n      <td>[175.0, 123.5, 27.0, 40.0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(f'Data contain {len(data.imageid.unique())} frame \\nMaximum No. objects founded in a frame is: {data.imageid.value_counts().iloc[0]} objects')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:02:12.382170Z","iopub.execute_input":"2024-04-18T21:02:12.382889Z","iopub.status.idle":"2024-04-18T21:02:12.398375Z","shell.execute_reply.started":"2024-04-18T21:02:12.382857Z","shell.execute_reply":"2024-04-18T21:02:12.397180Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Data contain 32 frame \nMaximum No. objects founded in a frame is: 5 objects\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.bar(data['label'].unique(), data.label.value_counts().tolist())","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:02:13.227995Z","iopub.execute_input":"2024-04-18T21:02:13.228366Z","iopub.status.idle":"2024-04-18T21:02:13.484623Z","shell.execute_reply.started":"2024-04-18T21:02:13.228336Z","shell.execute_reply":"2024-04-18T21:02:13.483787Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<BarContainer object of 5 artists>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"def prepare_bbox(ip):\n    return [ip[0], ip[1], ip[2], ip[3], ip[4], ip[5]]\nx, y, z, w, h, d = [], [], [], [], [], []\nfor u in df.bbox.apply(prepare_bbox):\n    x.append(u[0])\n    y.append(u[1])\n    z.append(u[2])\n    w.append(u[3])\n    h.append(u[2])\n    d.append(u[3])\ndata = pd.DataFrame()\ndata = df\ndata['x'] = x\ndata['y'] = y\ndata['z'] = z\ndata['w'] = w\ndata['h'] = h\ndata['d'] = d\ndata.drop(['bbox'], axis=1, inplace =True)\ndata.head(2)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-18T20:58:56.859091Z","iopub.execute_input":"2024-04-18T20:58:56.859366Z","iopub.status.idle":"2024-04-18T20:58:56.898545Z","shell.execute_reply.started":"2024-04-18T20:58:56.859342Z","shell.execute_reply":"2024-04-18T20:58:56.897080Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [ip[\u001b[38;5;241m0\u001b[39m], ip[\u001b[38;5;241m1\u001b[39m], ip[\u001b[38;5;241m2\u001b[39m], ip[\u001b[38;5;241m3\u001b[39m], ip[\u001b[38;5;241m4\u001b[39m], ip[\u001b[38;5;241m5\u001b[39m]]\n\u001b[1;32m      3\u001b[0m x, y, z, w, h, d \u001b[38;5;241m=\u001b[39m [], [], [], [], [], []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mbbox\u001b[38;5;241m.\u001b[39mapply(prepare_bbox):\n\u001b[1;32m      5\u001b[0m     x\u001b[38;5;241m.\u001b[39mappend(u[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      6\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(u[\u001b[38;5;241m1\u001b[39m])\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"],"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# Detectron2","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-04-18T20:58:56.904118Z","iopub.execute_input":"2024-04-18T20:58:56.904407Z","iopub.status.idle":"2024-04-18T20:58:57.967858Z","shell.execute_reply.started":"2024-04-18T20:58:56.904383Z","shell.execute_reply":"2024-04-18T20:58:57.966637Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Thu Apr 18 20:58:57 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   39C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   41C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())","metadata":{"execution":{"iopub.status.busy":"2024-04-18T20:58:57.969911Z","iopub.execute_input":"2024-04-18T20:58:57.970217Z","iopub.status.idle":"2024-04-18T20:59:03.982824Z","shell.execute_reply.started":"2024-04-18T20:58:57.970190Z","shell.execute_reply":"2024-04-18T20:59:03.981924Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"2.1.2 True\n","output_type":"stream"}]},{"cell_type":"code","source":"!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'","metadata":{"execution":{"iopub.status.busy":"2024-04-18T20:59:03.983940Z","iopub.execute_input":"2024-04-18T20:59:03.984547Z","iopub.status.idle":"2024-04-18T21:01:05.935982Z","shell.execute_reply.started":"2024-04-18T20:59:03.984520Z","shell.execute_reply":"2024-04-18T21:01:05.934822Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/facebookresearch/detectron2.git\n  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-h_5csaut\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-h_5csaut\n  Resolved https://github.com/facebookresearch/detectron2.git to commit a59f05630a8f205756064244bf5beb8661f96180\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: Pillow>=7.1 in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (9.5.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (3.7.5)\nCollecting pycocotools>=2.0.2 (from detectron2==0.6)\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (2.4.0)\nCollecting yacs>=0.1.8 (from detectron2==0.6)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (0.9.0)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (2.2.1)\nRequirement already satisfied: tqdm>4.29.0 in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (4.66.1)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (2.15.1)\nCollecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\nCollecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nCollecting hydra-core>=1.1 (from detectron2==0.6)\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nCollecting black (from detectron2==0.6)\n  Downloading black-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (76 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from detectron2==0.6) (21.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\nCollecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black->detectron2==0.6) (8.1.7)\nRequirement already satisfied: mypy-extensions>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from black->detectron2==0.6) (1.0.0)\nCollecting packaging (from detectron2==0.6)\n  Downloading packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\nCollecting pathspec>=0.9.0 (from black->detectron2==0.6)\n  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black->detectron2==0.6) (4.2.0)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black->detectron2==0.6) (2.0.1)\nRequirement already satisfied: typing-extensions>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from black->detectron2==0.6) (4.9.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (3.5.2)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->detectron2==0.6) (3.0.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\nDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\nDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading black-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.0-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\nDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n  Building wheel for detectron2 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=1254204 sha256=ba750efcfdd992c70f928500a520ddafc60a556f42988da6ae19762fb1089715\n  Stored in directory: /tmp/pip-ephem-wheel-cache-1mk17mb4/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=1fee52b1c818426fbcde1ed87650f2fa042fcf9b8c420cec7751e62b50751707\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=97c10e76a891acbc3a8392784679f0692b7aab6a2a9633a7bdbe4d05cb4da150\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\nSuccessfully built detectron2 fvcore antlr4-python3-runtime\nInstalling collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, packaging, omegaconf, iopath, hydra-core, black, pycocotools, fvcore, detectron2\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.0 which is incompatible.\njupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 black-24.4.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 omegaconf-2.3.0 packaging-24.0 pathspec-0.12.1 portalocker-2.8.2 pycocotools-2.0.7 yacs-0.1.8\n","output_type":"stream"}]},{"cell_type":"code","source":"!python -m detectron2.utils.collect_env","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:01:05.938588Z","iopub.execute_input":"2024-04-18T21:01:05.938903Z","iopub.status.idle":"2024-04-18T21:01:19.440052Z","shell.execute_reply.started":"2024-04-18T21:01:05.938876Z","shell.execute_reply":"2024-04-18T21:01:19.439087Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"-------------------------------  ------------------------------------------------------------------------------\nsys.platform                     linux\nPython                           3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\nnumpy                            1.26.4\ndetectron2                       0.6 @/opt/conda/lib/python3.10/site-packages/detectron2\nCompiler                         GCC 9.4\nCUDA compiler                    CUDA 12.1\ndetectron2 arch flags            7.5\nDETECTRON2_ENV_MODULE            <not set>\nPyTorch                          2.1.2 @/opt/conda/lib/python3.10/site-packages/torch\nPyTorch debug build              False\ntorch._C._GLIBCXX_USE_CXX11_ABI  True\nGPU available                    Yes\nGPU 0,1                          Tesla T4 (arch=7.5)\nDriver version                   535.129.03\nCUDA_HOME                        /usr/local/cuda\nPillow                           9.5.0\ntorchvision                      0.16.2 @/opt/conda/lib/python3.10/site-packages/torchvision\ntorchvision arch flags           6.0, 7.0, 7.5\nfvcore                           0.1.5.post20221221\niopath                           0.1.9\ncv2                              4.9.0\n-------------------------------  ------------------------------------------------------------------------------\nPyTorch built with:\n  - GCC 9.4\n  - C++ Version: 201703\n  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications\n  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n  - LAPACK is enabled (usually provided by MKL)\n  - NNPACK is enabled\n  - CPU capability usage: AVX2\n  - CUDA Runtime 12.1\n  - NVCC architecture flags: -gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_70,code=compute_70;-gencode;arch=compute_75,code=compute_75\n  - CuDNN 8.9\n  - Magma 2.6.1\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.0, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n\nTesting NCCL connectivity ... this should not hang.\nNCCL succeeded.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## libraries","metadata":{}},{"cell_type":"code","source":"# import some common libraries\nimport numpy as np\nimport os, json, cv2, random\nimport skimage.io as io\nimport pylab\nimport sys\nimport copy\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import colors\nfrom tensorboard.backend.event_processing import event_accumulator as ea\nfrom PIL import Image\n\n\n# Setup detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.utils.visualizer import ColorMode\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\n\n# Set base params\n#plt.rcParams[\"figure.figsize\"] = [16,9]","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:01:19.441408Z","iopub.execute_input":"2024-04-18T21:01:19.441685Z","iopub.status.idle":"2024-04-18T21:01:22.880935Z","shell.execute_reply.started":"2024-04-18T21:01:19.441659Z","shell.execute_reply":"2024-04-18T21:01:22.880151Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"dataset_dir = \".../kaggle/input/carrada\"\ntrain_dir = \"/test\"","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:01:22.882226Z","iopub.execute_input":"2024-04-18T21:01:22.882919Z","iopub.status.idle":"2024-04-18T21:01:22.887944Z","shell.execute_reply.started":"2024-04-18T21:01:22.882885Z","shell.execute_reply":"2024-04-18T21:01:22.886864Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class preprocessing:\n    def magnitude(self, target_array):\n        \"\"\" get magnitude out of complex number \"\"\"\n        target_array = np.abs(target_array)\n        target_array = pow(target_array, 2)\n        return target_array \n\n    def log(self, target_array):\n        \"\"\" get Log values \"\"\"\n        return  10 * np.log10(target_array + 1.)\n\n\n    def summation(self, target_array, target_axis):\n        \"\"\" sum up one dimension \"\"\"\n        output = np.sum(target_array, axis=target_axis)\n        return output\n    \n    def process(self, spectrum):\n        \"\"\" Return spectrum \"\"\"\n        return self.complex_to_channel(spectrum)\n    \n    def complex_to_channel(self, target_array):\n        \"\"\" transfer complex a + bi to [a, b] \"\"\"\n        assert target_array.dtype == np.complex64\n        output_array = self.magnitude(target_array)\n        output_array = self.log(output_array)\n        return output_array\n    \n    def extract_RA(self, target_array):\n        \"\"\" Project 3D into 2D \"\"\"\n        return self.log(self.summation(self.magnitude(target_array), target_axis=-1))","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:01:22.889304Z","iopub.execute_input":"2024-04-18T21:01:22.889769Z","iopub.status.idle":"2024-04-18T21:01:22.903279Z","shell.execute_reply.started":"2024-04-18T21:01:22.889727Z","shell.execute_reply":"2024-04-18T21:01:22.902344Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from detectron2.data import MetadataCatalog, DatasetCatalog\n\nclass data(preprocessing):\n    def __init__(self, df):\n        self.df = df\n        self.ids =  df.imageid.unique().tolist()\n        self.find_min_max()\n\n        \n    def find_min_max(self):\n        \"\"\" Find min and max in whole data \"\"\"\n        global_min = np.inf\n        global_max = -np.inf\n        \n        for img_id in self.ids:\n            path = os.path.join('/kaggle/input/carrada/test/RAD', img_id + '.npy')\n            spectrum = np.load(path)\n            spectrum = self.process(spectrum)\n            temp_min = np.min(spectrum)\n            temp_max = np.max(spectrum)\n            if temp_max > global_max:\n                global_max = temp_max\n            if temp_min < global_min:\n                global_min = temp_min\n        self.global_min = global_min\n        self.global_max = global_max\n\n        \n    def transforms(self, spectrum):\n        \"\"\" Return normalized image \"\"\"\n        return (spectrum-self.global_min)/(self.global_max-self.global_min)\n    \n \n    def __len__(self):\n        \"\"\" Return number of frames \"\"\"\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        spectrum = np.load('/kaggle/input/carrada/test/RAD/' + str(self.ids[idx]) + '.npy')\n        \n        \n        \n        # NOTE: Gloabl Normalization\n        spectrum = self.extract_RA(spectrum)\n#         spectrum = self.process(spectrum)     #for RAD\n        spectrum = self.transforms(spectrum)\n        spectrum = torch.as_tensor(spectrum[None, :, :], dtype=torch.float32)\n#         spectrum = torch.from_numpy(spectrum)\n        labels = self.df['label'][self.df['imageid'] == self.ids[idx]].tolist()\n        labels = torch.as_tensor(labels, dtype=torch.int64)      \n        boxes = self.df['bbox'][self.df['imageid'] == self.ids[idx]].tolist()\n        boxes = torch.as_tensor(boxes)\n        \n        li = []\n        for x in range(len(labels)):\n            anna = {}\n            anna['bbox'] = boxes[x]\n            anna['category_id'] = labels[x]\n            anna[\"bbox_mode\"] = 1\n            li.append(anna)\n        \n        data_dict = {}\n        data_dict[\"image\"] = spectrum.float()\n#         data_dict[\"annotations\"] = li\n        data_dict[\"boxes\"] = boxes\n        data_dict[\"labels\"] = labels\n        data_dict[\"file_name\"] = '/kaggle/input/carrada/test/RAD/' + str(self.ids[idx]) + '.npy'\n        data_dict[\"width\"] = spectrum.shape[0]\n        data_dict[\"height\"] = spectrum.shape[1]\n        \n        \n        return data_dict\n    \ndef loader():\n    return data(df) ","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:02:58.915073Z","iopub.execute_input":"2024-04-18T21:02:58.915758Z","iopub.status.idle":"2024-04-18T21:02:58.930362Z","shell.execute_reply.started":"2024-04-18T21:02:58.915727Z","shell.execute_reply":"2024-04-18T21:02:58.929336Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"DatasetCatalog.register(\"train\", loader)\nmetadata_dicts = MetadataCatalog.get(\"train\").set(thing_classes=[\"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"truck\"])","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:02:36.757732Z","iopub.execute_input":"2024-04-18T21:02:36.758107Z","iopub.status.idle":"2024-04-18T21:02:36.763274Z","shell.execute_reply.started":"2024-04-18T21:02:36.758079Z","shell.execute_reply":"2024-04-18T21:02:36.762245Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"dataset_dicts = DatasetCatalog.get('train')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:03:01.624908Z","iopub.execute_input":"2024-04-18T21:03:01.625882Z","iopub.status.idle":"2024-04-18T21:03:18.884735Z","shell.execute_reply.started":"2024-04-18T21:03:01.625849Z","shell.execute_reply":"2024-04-18T21:03:18.883904Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"d = copy.deepcopy(dataset_dicts)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:03:18.886328Z","iopub.execute_input":"2024-04-18T21:03:18.886616Z","iopub.status.idle":"2024-04-18T21:03:18.891511Z","shell.execute_reply.started":"2024-04-18T21:03:18.886592Z","shell.execute_reply":"2024-04-18T21:03:18.890491Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"d[0]['image'].shape","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:03:43.425078Z","iopub.execute_input":"2024-04-18T21:03:43.425742Z","iopub.status.idle":"2024-04-18T21:03:43.466332Z","shell.execute_reply.started":"2024-04-18T21:03:43.425711Z","shell.execute_reply":"2024-04-18T21:03:43.465436Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 256, 256])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 2, figsize =(20,14))\nindices=[ax[0][0],ax[1][0],ax[0][1],ax[1][1] ]\ni=-1\nfor d in random.sample(dataset_dicts, 4):\n    i=i+1    \n#     img = cv2.imread(d[\"image_id\"])\n    spectrum = np.load('')\n    v = Visualizer(img[:, :, ::-1],\n                   metadata=metadata_dicts, \n                   scale=0.4, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n    )\n    out = v.draw_dataset_dict(d)\n    indices[i].grid(False)\n    indices[i].axis('off')\n    indices[i].imshow(out.get_image()[:, :, ::-1])","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:01:23.543797Z","iopub.status.idle":"2024-04-18T21:01:23.544165Z","shell.execute_reply.started":"2024-04-18T21:01:23.543992Z","shell.execute_reply":"2024-04-18T21:01:23.544008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\nfrom detectron2.data import detection_utils as utils\nimport detectron2.data.transforms as T\n\ndef custom_mapper(dataset_dict):\n#     dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n    # can use other ways to read image\n#     annos = [\n#         annotation\n#         for annotation in dataset_dict.pop(\"annotations\")\n#     ]\n    \n#     dataset_dict[\"instances\"] = utils.annotations_to_instances(annos, (256,256))\n   \n    return dataset_dict\nclass AugTrainer(DefaultTrainer):\n    \n    @classmethod\n    def build_train_loader(cls, cfg):\n        return build_detection_train_loader(cfg, mapper=custom_mapper)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:03:49.466211Z","iopub.execute_input":"2024-04-18T21:03:49.466915Z","iopub.status.idle":"2024-04-18T21:03:49.472846Z","shell.execute_reply.started":"2024-04-18T21:03:49.466885Z","shell.execute_reply":"2024-04-18T21:03:49.471835Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"cfg = get_cfg()\nconfig_name = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\" \n# config_name = \"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"\ncfg.merge_from_file(model_zoo.get_config_file(config_name))\n\ncfg.DATASETS.TRAIN = (\"train\",)\n# cfg.DATASETS.TEST = (\"val\",)\ncfg.DATALOADER.NUM_WORKERS = 1\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_name)\n\ncfg.SOLVER.IMS_PER_BATCH = 4\ncfg.SOLVER.BASE_LR = 0.00025\n\ncfg.SOLVER.WARMUP_ITERS = 1000\ncfg.SOLVER.MAX_ITER = 3500  #adjust up if val mAP is still rising, adjust down if overfit\n#cfg.SOLVER.STEPS = (100, 500) # must be less than  MAX_ITER \n#cfg.SOLVER.GAMMA = 0.05\n\n\ncfg.SOLVER.CHECKPOINT_PERIOD = 100000  # Small value=Frequent save need a lot of storage.\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 5\n\n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n\n\n#Training using custom trainer defined above\ntrainer = AugTrainer(cfg) \n# trainer = DefaultTrainer(cfg) \ntrainer.resume_or_load(resume=False)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T21:04:01.926548Z","iopub.execute_input":"2024-04-18T21:04:01.926927Z","iopub.status.idle":"2024-04-18T21:04:05.399215Z","shell.execute_reply.started":"2024-04-18T21:04:01.926894Z","shell.execute_reply":"2024-04-18T21:04:05.397460Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"\u001b[32m[04/18 21:04:02 d2.engine.defaults]: \u001b[0mModel:\nGeneralizedRCNN(\n  (backbone): FPN(\n    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (top_block): LastLevelMaxPool()\n    (bottom_up): ResNet(\n      (stem): BasicStem(\n        (conv1): Conv2d(\n          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n      )\n      (res2): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n        )\n      )\n      (res3): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n        )\n      )\n      (res4): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (3): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (4): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n        (5): BottleneckBlock(\n          (conv1): Conv2d(\n            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n          )\n        )\n      )\n      (res5): Sequential(\n        (0): BottleneckBlock(\n          (shortcut): Conv2d(\n            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n          (conv1): Conv2d(\n            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (1): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n        (2): BottleneckBlock(\n          (conv1): Conv2d(\n            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv2): Conv2d(\n            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n          )\n          (conv3): Conv2d(\n            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n          )\n        )\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n  )\n  (roi_heads): StandardROIHeads(\n    (box_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (box_head): FastRCNNConvFCHead(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc_relu1): ReLU()\n      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n      (fc_relu2): ReLU()\n    )\n    (box_predictor): FastRCNNOutputLayers(\n      (cls_score): Linear(in_features=1024, out_features=6, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n    )\n  )\n)\n\u001b[32m[04/18 21:04:04 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n\u001b[32m[04/18 21:04:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n\u001b[32m[04/18 21:04:04 d2.data.common]: \u001b[0mSerializing 32 elements to byte tensors and concatenating them all ...\n\u001b[32m[04/18 21:04:04 d2.data.common]: \u001b[0mSerialized dataset takes 8.04 MiB\n\u001b[32m[04/18 21:04:04 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=4\n\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/18 21:04:04 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n\u001b[32m[04/18 21:04:04 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n\u001b[32m[04/18 21:04:05 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[04/18 21:04:05 d2.engine.train_loop]: \u001b[0mException during training:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/detectron2/engine/train_loop.py\", line 155, in train\n    self.run_step()\n  File \"/opt/conda/lib/python3.10/site-packages/detectron2/engine/defaults.py\", line 496, in run_step\n    self._trainer.run_step()\n  File \"/opt/conda/lib/python3.10/site-packages/detectron2/engine/train_loop.py\", line 310, in run_step\n    loss_dict = self.model(data)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py\", line 161, in forward\n    proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/detectron2/modeling/proposal_generator/rpn.py\", line 470, in forward\n    assert gt_instances is not None, \"RPN requires gt_instances in training!\"\nAssertionError: RPN requires gt_instances in training!\n\u001b[32m[04/18 21:04:05 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:00 (0:00:00 on hooks)\n\u001b[32m[04/18 21:04:05 d2.utils.events]: \u001b[0m iter: 0       lr: N/A  max_mem: 784M\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# trainer = DefaultTrainer(cfg) \u001b[39;00m\n\u001b[1;32m     31\u001b[0m trainer\u001b[38;5;241m.\u001b[39mresume_or_load(resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/engine/defaults.py:486\u001b[0m, in \u001b[0;36mDefaultTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    480\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m    Run training.\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03m        OrderedDict of results, if evaluation is enabled. Otherwise None.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mTEST\u001b[38;5;241m.\u001b[39mEXPECTED_RESULTS) \u001b[38;5;129;01mand\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mis_main_process():\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_last_eval_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo evaluation results obtained during training!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/engine/train_loop.py:155\u001b[0m, in \u001b[0;36mTrainerBase.train\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_iter, max_iter):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore_step()\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_step()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# self.iter == max_iter can be used by `after_train` to\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# tell whether the training successfully finished or failed\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# due to exceptions.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/engine/defaults.py:496\u001b[0m, in \u001b[0;36mDefaultTrainer.run_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/engine/train_loop.py:310\u001b[0m, in \u001b[0;36mSimpleTrainer.run_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    307\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03mIf you want to do something with the losses, you can wrap the model.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss_dict, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    312\u001b[0m     losses \u001b[38;5;241m=\u001b[39m loss_dict\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py:161\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    158\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(images\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproposal_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_instances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/proposal_generator/rpn.py:470\u001b[0m, in \u001b[0;36mRPN.forward\u001b[0;34m(self, images, features, gt_instances)\u001b[0m\n\u001b[1;32m    461\u001b[0m pred_anchor_deltas \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# (N, A*B, Hi, Wi) -> (N, A, B, Hi, Wi) -> (N, Hi, Wi, A, B) -> (N, Hi*Wi*A, B)\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_generator\u001b[38;5;241m.\u001b[39mbox_dim, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m pred_anchor_deltas\n\u001b[1;32m    467\u001b[0m ]\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 470\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m gt_instances \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRPN requires gt_instances in training!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m     gt_labels, gt_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_and_sample_anchors(anchors, gt_instances)\n\u001b[1;32m    472\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses(\n\u001b[1;32m    473\u001b[0m         anchors, pred_objectness_logits, gt_labels, pred_anchor_deltas, gt_boxes\n\u001b[1;32m    474\u001b[0m     )\n","\u001b[0;31mAssertionError\u001b[0m: RPN requires gt_instances in training!"],"ename":"AssertionError","evalue":"RPN requires gt_instances in training!","output_type":"error"}]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# torch","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:30:42.704462Z","iopub.execute_input":"2024-04-11T18:30:42.704780Z","iopub.status.idle":"2024-04-11T18:30:42.709063Z","shell.execute_reply.started":"2024-04-11T18:30:42.704759Z","shell.execute_reply":"2024-04-11T18:30:42.707973Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"class CusDat(torch.utils.data.Dataset):\n    def __init__(delf, df, unique_imgs, indices):\n        self.df = df\n        self.unique_imgs = unique_imgs\n        self.indices = indices\n    def __len__(self):\n        return len(self.indices)\n    def __getitem__(self, idx):\n        image_name = self.unique_imgs[self.indices[idx]]\n        boxes = self.df[self.df.image_id == image_name].values[:, 1:].astype('float')\n        img = Image.apen(\".../kaggle/input/carrada/test/stereo_image/\"+ image_name + \".jpg\").convert('RGB')\n        labels = torch.ones((boxes.shape[0]), dtype = torch.int64)\n        target = {}\n        target['boxes'] = torch.tensor(boxes)\n        target['label'] = labels\n        return torchvision.transforms.ToTensor()(img), target","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\nnum_classes = 7\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# YOLO","metadata":{}},{"cell_type":"code","source":"! git clone https://github.com/ultralytics/yolov5.git\n! cd yolov5\n! pip install -r requirements.txt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! python train.py --img 640 --batch 16 --epochs 50 --data data.yaml --cfg models/yolov5s.yaml --weights '' --name custom_dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! python val.py --img 640 --data data.yaml --weights runs/train/custom_dataset/weights/best.pt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! python train.py --img 640 --batch 16 --epochs 100 --data data.yaml --cfg models/yolov5s.yaml --weights runs/train/custom_dataset/weights/last.pt --name custom_dataset_ft","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# TensorFlow","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:41:37.721101Z","iopub.execute_input":"2024-04-16T19:41:37.721838Z","iopub.status.idle":"2024-04-16T19:41:49.062986Z","shell.execute_reply.started":"2024-04-16T19:41:37.721808Z","shell.execute_reply":"2024-04-16T19:41:49.062097Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-04-16 19:41:39.444774: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-16 19:41:39.444887: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-16 19:41:39.580280: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# parameters from the paper: Range-Azimuth-Doppler based Radar Object Detectionfor Dynamic Road Users paper \n# These param. can't be used because TINY PORTION of dataset will be trained\n# parameters = {\"global_mean_log\" : 3.2438383, \"global_max_log\" : 10.0805629, \"global_min_log\" : 0.0, \"global_variance_log\" : 6.8367246, \"designed_frequency\" : 76.8,\"config_frequency\" : 77,\"range_size\" : 256,\"doppler_size\" : 64,\"azimuth_size\" : 256,\"range_resolution\" : 0.1953125,\"angular_resolution\" : 0.006135923,\"velocity_resolution\" : 0.41968030701528203}","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:41:49.064562Z","iopub.execute_input":"2024-04-16T19:41:49.065073Z","iopub.status.idle":"2024-04-16T19:41:49.073042Z","shell.execute_reply.started":"2024-04-16T19:41:49.065047Z","shell.execute_reply":"2024-04-16T19:41:49.071946Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def readRAD(filename):\n    \"\"\" read input RAD matrices \"\"\"\n    if os.path.exists(filename):\n        return np.load(filename)\n    else:\n        return None\n\ndef gtfileFromRADfile(RAD_file, prefix):\n    \"\"\" Transfer RAD filename to gt filename \"\"\"\n    RAD_file_spec = RAD_file.split(\"RAD\")[-1]\n    gt_file = os.path.join(prefix, \"gt\") + RAD_file_spec.replace(\"npy\", \"pickle\")\n    return gt_file\n \ndef imgfileFromRADfile(RAD_file, prefix):\n    \"\"\" Transfer RAD filename to gt filename \"\"\"\n    RAD_file_spec = RAD_file.split(\"RAD\")[-1]\n    gt_file = os.path.join(prefix, \"stereo_image\") + RAD_file_spec.replace(\"npy\", \"jpg\")\n    return gt_file\n\n\n\ndef readRadarInstances(pickle_file):\n    \"\"\" read output radar instances. \"\"\"\n    if os.path.exists(pickle_file):\n        with open(pickle_file, \"rb\") as f:\n            radar_instances = pickle.load(f)\n        if len(radar_instances['classes']) == 0:\n            radar_instances = None\n    else:\n        radar_instances = None\n    return radar_instances\n   \ndef readStereoLeft(img_filename):\n    \"\"\" read stereo left image for verification. \"\"\"\n    if os.path.exists(img_filename):\n        stereo_image = cv2.imread(img_filename)\n        left_image = stereo_image[:, :stereo_image.shape[1]//2, ...][..., ::-1]\n        return left_image\n    else:\n        return None\n    \ndef readSingleImage(img_filename):\n    \"\"\" read stereo left image for verification. \"\"\"\n    if os.path.exists(img_filename):\n        image = cv2.imread(img_filename)\n        image = image[..., ::-1]\n        return image\n    else:\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:41:49.074773Z","iopub.execute_input":"2024-04-16T19:41:49.075091Z","iopub.status.idle":"2024-04-16T19:41:49.098000Z","shell.execute_reply.started":"2024-04-16T19:41:49.075065Z","shell.execute_reply":"2024-04-16T19:41:49.097132Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def normalization(file_name):\n    RAD_data = np.load(file_name)\n    RAD_complex = readRAD(RAD_filename)\n    # normalize whole data\n    RAD_data = complexTo2Channels(RAD_complex)\n    normalized_RAD = (RAD_data - parameteres[\"global_mean_log\"]) / parameteres[\"global_variance_log\"]\n    \n    # return normalized data\n    return normalized_RAD\n\n\ndef cartesian_to_polar(x, y):\n    \"\"\" Cartesian to Polar \"\"\"\n    rho = np.sqrt(x**2 + y**2)\n    phi = np.arctan2(y, x)\n    return(rho, phi)\n\ndef polar_to_cartesian(rho, phi):\n    \"\"\" Polar to Cartesian \"\"\"\n    x = rho * np.cos(phi)\n    y = rho * np.sin(phi)\n    return(x, y)\n\ndef complexTo2Channels(target_array):\n    \"\"\" transfer complex a + bi to [a, b]\"\"\"\n    assert target_array.dtype == np.complex64\n    ### NOTE: transfer complex to (magnitude) ###\n    output_array = magnitude(target_array)\n    output_array = log(output_array)\n    return output_array\n\ndef magnitude(target_array):\n    \"\"\" get magnitude out of complex number \"\"\"\n    target_array = np.abs(target_array)\n    target_array = pow(target_array, 2)\n    return target_array \n\ndef log(target_array):\n    \"\"\" get Log values \"\"\"\n    return  10 * np.log10(target_array + 1.)\n\n\ndef sum(target_array, target_axis):\n    \"\"\" sum up one dimension \"\"\"\n    output = np.sum(target_array, axis=target_axis)\n    return output\n\ndef extract_RA(file_name):\n    RAD = normalization(file_name).T\n    RA = np.log(np.abs(RAD).sum(axis=0))\n    return RA.T\n\ndef extract_RD(file_name):\n    RAD = normalization(file_name)\n    RD = np.log(np.abs(RAD).sum(axis=0))\n    return RD\n\ndef extract_DoA(file_name):\n    RA = extract_RA(file_name) \n    return polar_to_cartesian(RA)\n\ndef readRAD(filename):\n    if os.path.exists(filename):\n        return np.load(filename)\n    else:\n        return None\n    \n# RAD = readRAD('/kaggle/input/carrada/test/RAD/000156.npy')\n\n# RA = getLog(getSumDim(getMagnitude(RAD, power_order=2), target_axis=-1), scalar=10, log_10=True)\n# RD = getLog(getSumDim(getMagnitude(RAD, power_order=2), target_axis=1), scalar=10, log_10=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:42:16.928856Z","iopub.execute_input":"2024-04-16T19:42:16.929608Z","iopub.status.idle":"2024-04-16T19:42:16.942805Z","shell.execute_reply.started":"2024-04-16T19:42:16.929576Z","shell.execute_reply":"2024-04-16T19:42:16.941796Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":" def encodeToLabels(gt_instances):\n        \"\"\" Transfer ground truth instances into Detection Head format \"\"\"\n        raw_boxes_xyzwhd = np.zeros((30, 7)) # 30 is the max number of boxes in frame \n        \n        ### initialize gronud truth labels as np.zeors ###\n        gt_labels = np.zeros(list(self.headoutput_shape[1:4]) + [len(self.anchor_boxes)] + [len(classes[\"all_classes\"]) + 7])\n        \n        classes = {\"all_classes\" : [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"truck\" ]}\n        ### start transferring box to ground turth label format ###\n        for i in range(len(gt_instances[\"classes\"])):\n            if i > 30:\n                continue\n            class_name = gt_instances[\"classes\"][i]\n            box_xyzwhd = gt_instances[\"boxes\"][i]\n            class_id = classes[\"all_classes\"].index(class_name)\n            if i < 30:\n                raw_boxes_xyzwhd[i, :6] = box_xyzwhd\n                raw_boxes_xyzwhd[i, 6] = class_id\n            class_onehot = helper.smoothOnehot(class_id, len(self.config_data[\"all_classes\"]))\n            \n            exist_positive = False\n\n            grid_strid = np.array(np.array(self.config_model[\"input_shape\"])[:3] / np.array(self.headoutput_shape[1:4])).astype(np.float32)\n            anchor_stage = self.anchor_boxes\n            box_xyzwhd_scaled = box_xyzwhd[np.newaxis, :].astype(np.float32)\n            box_xyzwhd_scaled[:, :3] /= grid_strid\n            anchorstage_xyzwhd = np.zeros([len(anchor_stage), 6])\n            anchorstage_xyzwhd[:, :3] = np.floor(box_xyzwhd_scaled[:, :3]) + 0.5\n            anchorstage_xyzwhd[:, 3:] = anchor_stage.astype(np.float32)\n\n            iou_scaled = helper.iou3d(box_xyzwhd_scaled, anchorstage_xyzwhd, self.input_size)\n            ### NOTE: 0.3 is from YOLOv4, maybe this should be different here ###\n            ### it means, as long as iou is over 0.3 with an anchor, the anchor\n            ### should be taken into consideration as a ground truth label\n            iou_mask = iou_scaled > 0.3\n\n            if np.any(iou_mask):\n                xind, yind, zind = np.floor(np.squeeze(box_xyzwhd_scaled)[:3]).astype(np.int32)\n                ### TODO: consider changing the box to raw yolohead output format ###\n                gt_labels[xind, yind, zind, iou_mask, 0:6] = box_xyzwhd\n                gt_labels[xind, yind, zind, iou_mask, 6:7] = 1.\n                gt_labels[xind, yind, zind, iou_mask, 7:] = class_onehot\n                exist_positive = True\n\n            if not exist_positive:\n                ### NOTE: this is the normal one ###\n                ### it means take the anchor box with maximum iou to the raw\n                ### box as the ground truth label\n                anchor_ind = np.argmax(iou_scaled)\n                xind, yind, zind = np.floor(np.squeeze(box_xyzwhd_scaled)[:3]).astype(np.int32)\n                gt_labels[xind, yind, zind, anchor_ind, 0:6] = box_xyzwhd\n                gt_labels[xind, yind, zind, anchor_ind, 6:7] = 1.\n                gt_labels[xind, yind, zind, anchor_ind, 7:] = class_onehot\n\n        has_label = False\n        for label_stage in gt_labels:\n            if label_stage.max() != 0:\n                has_label = True\n        gt_labels = [np.where(gt_i == 0, 1e-16, gt_i) for gt_i in gt_labels]\n        return gt_labels, has_label, raw_boxes_xyzwhd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainData():\n    \"\"\" Generate train data with batch size \"\"\"\n    count = 0\n    paths =  glob.glob(os.path.join('/kaggle/input/carrada/test/', \"RAD/*/*.npy\"))'\n    while  count < len(paths):\n        RAD_filename = paths[count] \n        RAD_complex = readRAD(RAD_filename)\n\n        ### NOTE: Gloabl Normalization ###\n        RAD_data = helper.complexTo2Channels(RAD_complex)\n        RAD_data = (RAD_data - self.config_data[\"global_mean_log\"]) / self.config_data[\"global_variance_log\"]\n        \n        ### load ground truth instances ###\n        gt_instances = readRadarInstances(gtfileFromRADfile(RAD_filename, '/kaggle/input/carrada/test/'))\n        \n        ### NOTE: decode ground truth boxes to YOLO format ###\n        gt_labels, has_label, raw_boxes = encodeToLabels(gt_instances)\n\n        if has_label:\n            yield (RAD_data, gt_labels, raw_boxes)\n            \n        count += 1\n        if count == len(paths) - 1:\n            np.random.shuffle(paths)\n\ndef trainGenerator():\n    \"\"\" Building data generator using tf.data.Dataset.from_generator \"\"\"\n    return tf.data.Dataset.from_generator(self.trainData, output_types=(tf.float32, tf.float32, tf.float32), output_shapes=(256, 256, 64), tf.TensorShape(list(self.headoutput_shape[1:4]) + [len(self.anchor_boxes), 7+len(self.config_data[\"all_classes\"])]), tf.TensorShape(30, 7]) ), )\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process(RAD_filename):\n    prefix = '/kaggle/input/carrada/test'\n    RAD = readRAD(RAD_filename)\n    gt_instances = readRadarInstances(gtfileFromRADfile(RAD_filename, prefix))\n    stereo_left_image = loader.readStereoLeft(imgfileFromRADfile(RAD_filename, prefix))\n    if RAD is not None and gt_instances is not None and stereo_left_image is not None:\n        RA = log(summation(magnitude(RAD), target_axis=-1))\n        RD = log(summation(magnitude(RAD), target_axis=1))\n    return RAD, gt_instances, stereo_left_image, RA, RD\n\n\nall_RAD_files = glob(os.path.join('/kaggle/input/carrada/test/', \"RAD/*/*.npy\"))\nprint('start')\nfor i in tqdm.tqdm(range(len(all_RAD_files[:5]))):\n    print(i)\n    RAD_filename = all_RAD_files[i]\n    RAD, gt_instances, stereo_left_image, RA, RD = process(RAD_filename)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:46:41.778072Z","iopub.execute_input":"2024-04-16T19:46:41.778541Z","iopub.status.idle":"2024-04-16T19:46:41.791222Z","shell.execute_reply.started":"2024-04-16T19:46:41.778511Z","shell.execute_reply":"2024-04-16T19:46:41.790155Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"start\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"train = trainGenerator","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# YoloV5","metadata":{}},{"cell_type":"code","source":"!mkdir '/kaggle/working/data/label'","metadata":{"execution":{"iopub.status.busy":"2024-04-16T22:46:32.305687Z","iopub.execute_input":"2024-04-16T22:46:32.306457Z","iopub.status.idle":"2024-04-16T22:46:33.311581Z","shell.execute_reply.started":"2024-04-16T22:46:32.306409Z","shell.execute_reply":"2024-04-16T22:46:33.310158Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame()\ndf['format'] = data.imageid\n\n\nfor i in range(df.shape[0]):\n    df.loc[i, 'format'] = '/kaggle/input/carrada/test/RAD/' + df['format'][i]\nx = data.imageid[0]\nfor num_findings in data.imageid.value_counts().loc[x]:\n    for img in range(df.shape[0]):\n        file_name = df.loc[img, 'format']\n        dx = pd.DataFrame()\n        dx['format'][num_findings] = df['format'][img] + ',' + str(data.x[img]) + ',' + str(data.y[img]) + ',' + str(data.z[img]) + ',' + str(data.w[img]) + ',' + str(data.h[img]) + ',' + str(data.d[img]) + ',' + str(data.label[img]) \n        print(dx)\n        x = data.imageid[i]\n        i+=1\n        break\n    dx.to_csv('/kaggle/working/data/label/' + file_name.split('/')[-1] + '.txt', header=None, index=None, sep=' ')","metadata":{"execution":{"iopub.status.busy":"2024-04-16T23:09:24.127903Z","iopub.execute_input":"2024-04-16T23:09:24.128764Z","iopub.status.idle":"2024-04-16T23:09:24.154207Z","shell.execute_reply.started":"2024-04-16T23:09:24.128731Z","shell.execute_reply":"2024-04-16T23:09:24.152897Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"                                              format\n0  /kaggle/input/carrada/test/RAD/000090,146.5,10...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}}]}